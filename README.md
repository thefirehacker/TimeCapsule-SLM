<div align="center">
  
  <img src="https://mybubblpublic.s3.ap-south-1.amazonaws.com/TimeCapsule_03.png" alt="TimeCapsule-SLM Logo" width="180" style="border-radius: 20px; box-shadow: 0 10px 30px rgba(0,0,0,0.3);">
  
  # ğŸ’Š TimeCapsule-SLM
  
  ### *Complete AI-powered research & creative platform with DeepResearch*
  ### **Generate Novel Ideas â€¢ Build AI Content â€¢ Enable Collaborative Knowledge Discovery**
  
  [![Live Demo](https://img.shields.io/badge/ğŸš€_Live_Demo-Available-00C851?style=for-the-badge&logo=rocket)](https://timecapsule.bubblspace.com/)
  [![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg?style=for-the-badge)](https://opensource.org/licenses/Apache-2.0)
  [![AI Powered](https://img.shields.io/badge/ğŸ¤–_AI-Powered-ff6b35?style=for-the-badge)](https://timecapsule.bubblspace.com/)
  [![Made with Love](https://img.shields.io/badge/Made_with-â¤-red?style=for-the-badge)](https://x.com/thefirehacker)
  
</div>

---

## ğŸ”— **Quick Links**
- [ğŸš€ Key Features](#-key-features-2024)
- [ğŸŒ Experience Live Now](#-experience-live-now)
- [ğŸš¦ How to Start](#-how-to-start)
- [ğŸ³ Docker Support- In Progress](DOCKER.md)
- [ğŸ”— How to Share](#-how-to-share)
- [ğŸ’¬ Join Discord Community](https://discord.gg/ExQ8fCv9)
- [ğŸ¦™ Ollama Integration](#-ollama-integration)
- [ğŸ  LM Studio Integration](#-lm-studio-integration)
- [ğŸ”¬ DeepResearch Usage](#-deepresearch-timecapsule-usage)
- [ğŸ® Playground Usage](#-playground-usage)
- [ğŸ“„ Project Structure](#-project-structure)
- [ğŸš€ Powered By](#-powered-by)

---

## ğŸš€ **Key Features **

<div align="center">

### ğŸ§  **In-Browser RAG** | ğŸ”— **TimeCapsule Sharing** | ğŸ“š **Knowledge Base** | ğŸ¤– **Local LLM Support**

</div>

<table>
<tr>
<td width="50%">

### ğŸ§  **In-Browser RAG**
Semantic search and Retrieval-Augmented Generation with your own documentsâ€”directly in your browser. **No server, no data leaves your device.**

### ğŸ“š **Knowledge Base Integration**
Upload PDFs, text, and images to build a private, searchable research knowledge base with intelligent document analysis.

</td>
<td width="50%">

### ğŸ”— **TimeCapsule Sharing**
Export and load full research sessions as `.timecapsule.json` files. **Instantly share, restore, or collaborate** on research with anyone.

### ğŸ¤– **Local LLM Support**
Use **Ollama**, **LM Studio**, **OpenAI**, and **Anthropic**â€”privacy-first, cost-effective, and lightning fast.

</td>
</tr>
</table>

---

## ğŸŒ **âœ¨ Experience Live Now! âœ¨**

<div align="center">

### **ğŸ¯ [Launch TimeCapsule-SLM â†’](https://timecapsule.bubblspace.com/)**

<table>
<tr>
<td width="50%" align="center">
  
**ğŸš€ Instant Access**

No downloads, no setup - just click and create!
Professional-grade research and creative coding in your browser.
  
</td>
<td width="50%" align="center">
  
**ğŸ¤– AI-Powered**

**Ollama**, **LM Studio** & **API** (**OpenAI**, **Anthropic**) integration for intelligent research analysis and creative code generation.
  
</td>
</tr>
</table>

### ğŸ’Š **What You Get:**

ğŸ”¬ **DeepResearch TimeCapsule** - Comprehensive AI-powered research platform  
ğŸ® **Playground** - Execute TimeCapsules with creative coding  
ğŸ§  **Triple AI Mode**: Ollama, LM Studio and APIs (OpenAI, Anthropic)  
âš™ï¸ **Custom Context Templates** for personalized AI behavior  
ğŸ“± **Responsive Design** that works on all devices  
ğŸ”„ **Seamless Navigation** between research and creative modes  
ğŸ”’ **Privacy First** with multiple local AI options  

</div>

---

## ğŸš¦ **How to Start**

<div align="center">

### ğŸ¯ **Get Research-Ready in 5 Minutes**

</div>

<table>
<tr>
<td width="33%">

### ğŸŒ **Option 1: Instant Online** *(Recommended)*
1. ğŸ¯ **Go to** [timecapsule.bubblspace.com](https://timecapsule.bubblspace.com/)
2. ğŸ”¬ **Click "DeepResearch"**
3. ğŸ¦™ **Start Ollama** (see setup below)
4. ğŸ¤– **Pull a model:** `ollama pull qwen3:0.6b`
5. ğŸ“š **Add documents** in Knowledge Manager
6. ğŸ“ **Add research topics** and click **Generate**

</td>
<td width="33%">

### ğŸ³ **Option 2: Docker** *(Easy Deploy)*
1. ğŸ“ **Clone:** `git clone https://github.com/thefirehacker/TimeCapsule-SLM`
2. ğŸ“‚ **Navigate:** `cd TimeCapsule-SLM`
3. ğŸ³ **Start:** `docker-compose --profile ai-enabled up -d`
4. ğŸŒ **Access:** `http://localhost:3000`
5. ğŸ¦™ **Pull model:** `docker exec timecapsule-ollama ollama pull qwen3:0.6b`
6. ğŸš€ **Start researching!**

</td>
<td width="33%">

### ğŸ’» **Option 3: Local Development**
1. ğŸ“ **Clone:** `git clone https://github.com/thefirehacker/TimeCapsule-SLM`
2. ğŸ“‚ **Navigate:** `cd TimeCapsule-SLM`
3. ğŸŒ **Open:** `DeepResearch.html` in browser
4. ğŸ¦™ **Setup Ollama** (see integration guide)
5. ğŸš€ **Start researching!**

</td>
</tr>
</table>

### ğŸ¦™ **Quick Ollama Setup** *(Essential for local AI)*

```bash
# 1. Install Ollama from https://ollama.ai

# 2. Pull recommended model
ollama pull qwen3:0.6b

# 3. Start with CORS enabled (CRITICAL)
OLLAMA_ORIGINS="https://timecapsule.bubblspace.com/,http://localhost:3000" ollama serve

# 4. Connect in TimeCapsule-SLM
```

> **ğŸ’¡ Pro Tip:** For best results, use **Ollama** with the `qwen3:0.6b` model. **LM Studio** and **APIs** (OpenAI, Anthropic) are also fully supported.

---

## ğŸ”— **How to Share**

<div align="center">

### ğŸ¤ **Collaborate & Share Research Instantly**

</div>

<table>
<tr>
<td width="50%" align="center">

### ğŸ“¤ **Export TimeCapsule**
1. ğŸ”¬ **Complete your research** in DeepResearch
2. ğŸ’¾ **Click "Export TimeCapsule"** button
3. ğŸ“ **Save** `.timecapsule.json` file
4. ğŸ¤ **Share** with colleagues or save for later

**Perfect for:** Research collaboration, session backup, knowledge sharing

</td>
<td width="50%" align="center">

### ğŸ“¥ **Load TimeCapsule** 
1. ğŸ“‚ **Click "Load TimeCapsule"** button
2. ğŸ—‚ï¸ **Select** `.timecapsule.json` file
3. âš¡ **Instantly restore** topics and research output
4. ğŸ”„ **Continue** where you left off

**Perfect for:** Resuming sessions, importing shared research, team collaboration

</td>
</tr>
</table>

### ğŸ¯ **TimeCapsule Features**
- **ğŸ”„ Complete Session Restore** - All topics, research results, and notes
- **ğŸ“Š Multi-Tab Support** - Research, Sources, and Notes tabs preserved
- **ğŸ¤ Team Collaboration** - Share research across teams instantly
- **ğŸ’¾ Session Backup** - Never lose your research progress
- **ğŸŒ Cross-Platform** - Works on any device with TimeCapsule-SLM

---

## ğŸ¦™ **Ollama Integration**

<div align="center">

### ğŸ¯ **Local AI Power + Privacy First**
**Complete platform-specific setup guides for macOS, Linux & Windows**

</div>

<table>
<tr>
<td width="50%">

### ğŸš€ **Why Ollama?**
- **ğŸ”’ Fully Private** - All processing happens locally
- **ğŸ’° Zero API Costs** - No usage fees or limits
- **âš¡ Lightning Fast** - Optimized GGUF models
- **ğŸ›ï¸ Model Library** - Easy model management
- **ğŸŒ REST API** - Simple integration

</td>
<td width="50%">

### ğŸ› ï¸ **Setup Requirements**
- **Ollama App** - Download from [ollama.ai](https://ollama.ai)
- **AI Model** - Any compatible GGUF model
- **CORS Enabled** - **CRITICAL** for web access
- **Port 11434** - Default Ollama server port

</td>
</tr>
</table>

---

## ğŸ **macOS Setup Guide**

### ğŸ“¥ **Step 1: Install Ollama**
```bash
# Method 1: Direct download (recommended)
# Download from https://ollama.ai and install .app

# Method 2: Homebrew
brew install ollama
```

### ğŸ¤– **Step 2: Pull a Model**
```bash
# Recommended: Fast and efficient
ollama pull qwen3:0.6b
```

### ğŸ”§ **Step 3: Start with CORS** (**CRITICAL**)
```bash
# Kill any existing processes first
pkill -f ollama

# Start with CORS enabled (for testing)
OLLAMA_ORIGINS="*" ollama serve

# For production (recommended)
OLLAMA_ORIGINS="https://timecapsule.bubblspace.com/,http://localhost:3000" ollama serve
```

### ğŸ”§ **macOS Troubleshooting**

**âŒ "Operation not permitted" Error:**
```bash
# Method 1: Use sudo
sudo pkill -f ollama

# Method 2: Activity Monitor (GUI)
# 1. Open Activity Monitor (Applications â†’ Utilities)
# 2. Search for "ollama"
# 3. Select process and click "Force Quit"

# Method 3: Homebrew service (if installed via brew)
brew services stop ollama
brew services start ollama
```

**âŒ CORS Issues:**
```bash
# 1. Stop Ollama completely
sudo pkill -f ollama

# 2. Wait 3 seconds
sleep 3

# 3. Start with CORS
OLLAMA_ORIGINS="*" ollama serve

# 4. Test connection
curl http://localhost:11434/api/tags
```

---

## ğŸ§ **Linux Setup Guide**

### ğŸ“¥ **Step 1: Install Ollama**
```bash
# Official installer (recommended)
curl -fsSL https://ollama.ai/install.sh | sh

# Or download directly from https://ollama.ai
```

### ğŸ¤– **Step 2: Pull a Model**
```bash
# Recommended model
ollama pull qwen3:0.6b
```

### ğŸ”§ **Step 3: Configure CORS with systemctl** (**CRITICAL**)

**For systemd-based Linux distributions (Ubuntu, Debian, CentOS, etc.):**

```bash
# 1. Stop any running Ollama instances
ps aux | grep ollama
sudo pkill -f ollama

# 2. Edit the ollama service configuration
sudo systemctl edit ollama.service

# 3. Add the following environment variables:
[Service]
Environment="OLLAMA_HOST=0.0.0.0"
Environment="OLLAMA_ORIGINS=*"

# For production, use specific origins:
# Environment="OLLAMA_ORIGINS=https://timecapsule.bubblspace.com/,http://localhost:3000"

# 4. Save and exit the editor (Ctrl+X, then Y, then Enter)

# 5. Reload systemd and restart ollama service
sudo systemctl daemon-reload
sudo systemctl restart ollama.service

# 6. Enable auto-start on boot (optional)
sudo systemctl enable ollama.service

# 7. Verify the service is running
sudo systemctl status ollama.service

# 8. Test the connection
curl http://localhost:11434/api/tags
```

**Alternative: Manual start (if not using systemd):**
```bash
# Stop any existing processes
sudo pkill -f ollama

# Start manually with CORS
OLLAMA_ORIGINS="*" ollama serve

# Or for production:
# OLLAMA_ORIGINS="https://timecapsule.bubblspace.com/,http://localhost:3000" ollama serve
```

### ğŸ”§ **Linux Troubleshooting**

**âŒ Service Issues:**
```bash
# Check service logs
sudo journalctl -u ollama.service -f

# Restart service
sudo systemctl restart ollama.service

# Check service status
sudo systemctl status ollama.service
```

**âŒ Permission Issues:**
```bash
# Stop with elevated permissions
sudo pkill -f ollama

# Check for lingering processes
ps aux | grep ollama

# Force kill if needed
sudo kill -9 $(pgrep ollama)
```

**âŒ CORS Configuration:**
```bash
# Verify environment variables are set
sudo systemctl show ollama.service | grep Environment

# If not set, re-edit the service:
sudo systemctl edit ollama.service
# Add Environment variables as shown above
sudo systemctl daemon-reload
sudo systemctl restart ollama.service
```

**ğŸ“š Reference:** [Ollama CORS Configuration Guide](https://objectgraph.com/blog/ollama-cors/)

---

## ğŸªŸ **Windows Setup Guide**

### ğŸ“¥ **Step 1: Install Ollama**
```powershell
# Download from https://ollama.ai and install the .exe
# Or use package manager (if available)
```

### ğŸ¤– **Step 2: Pull a Model**
```powershell
# Open Command Prompt or PowerShell
ollama pull qwen3:0.6b
```

### ğŸ”§ **Step 3: Start with CORS** (**CRITICAL**)
```powershell
# Method 1: Stop existing processes
taskkill /f /im ollama.exe

# Method 2: Start with CORS (Command Prompt)
set OLLAMA_ORIGINS=* && ollama serve

# Method 3: Start with CORS (PowerShell)
$env:OLLAMA_ORIGINS="*"; ollama serve

# For production (specific origins):
# $env:OLLAMA_ORIGINS="https://timecapsule.bubblspace.com/,http://localhost:3000"; ollama serve
```

### ğŸ”§ **Windows Troubleshooting**

**âŒ Process Issues:**
```powershell
# Method 1: Task Manager (GUI)
# 1. Open Task Manager (Ctrl+Shift+Esc)
# 2. Look for "ollama.exe" in Processes tab
# 3. Right-click and select "End task"

# Method 2: Command line
taskkill /f /im ollama.exe

# Method 3: Find by port
netstat -ano | findstr :11434
# Note the PID and kill it:
taskkill /f /pid <PID>
```

**âŒ CORS Issues:**
```powershell
# 1. Stop all ollama processes
taskkill /f /im ollama.exe

# 2. Wait 3 seconds
timeout /t 3

# 3. Start with CORS
$env:OLLAMA_ORIGINS="*"; ollama serve

# 4. Test connection (if curl is available)
curl http://localhost:11434/api/tags
```

**âŒ Environment Variables:**
```powershell
# Set permanently (requires restart)
setx OLLAMA_ORIGINS "*"

# Set for current session only
$env:OLLAMA_ORIGINS="*"
```

---

## ğŸ¯ **Universal Commands & Verification**

### ğŸ§ª **Test Your Setup**
```bash
# 1. Check if Ollama is running
curl http://localhost:11434/api/tags

# 2. List installed models
ollama list

# 3. Test model response
curl http://localhost:11434/api/generate -d '{
  "model": "qwen3:0.6b",
  "prompt": "Hello",
  "stream": false
}'
```

### ğŸ“¦ **Recommended Models**

| Model | Size | Best For | Performance |
|-------|------|----------|-------------|
| **qwen3:0.6b** | ~400MB | Fast responses, testing | ğŸŒŸğŸŒŸğŸŒŸğŸŒŸâ­ |
| **qwen2.5:3b** | ~2GB | Balanced quality/speed | ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ |
| **llama3.2:3b** | ~2GB | General purpose | ğŸŒŸğŸŒŸğŸŒŸâ­â­ |

```bash
# Pull additional models:
ollama pull qwen2.5:3b
ollama pull llama3.2:3b
```

### ğŸ†˜ **Universal Reset (All Platforms)**

**If everything fails, complete reset:**
```bash
# 1. Stop all Ollama processes
# macOS/Linux: sudo pkill -f ollama
# Windows: taskkill /f /im ollama.exe

# 2. Wait 5 seconds
sleep 5  # macOS/Linux
# timeout /t 5  # Windows

# 3. Start fresh with CORS
OLLAMA_ORIGINS="*" ollama serve
# Windows PowerShell: $env:OLLAMA_ORIGINS="*"; ollama serve

# 4. Pull a model (in new terminal)
ollama pull qwen3:0.6b

# 5. Test setup
curl http://localhost:11434/api/tags
```

> **ğŸ’¡ Pro Tips**: 
> - **Linux Users:** Use systemctl for persistent CORS configuration
> - **macOS Users:** Use Activity Monitor for stubborn processes
> - **Windows Users:** Use Task Manager or PowerShell for process management
> - **All Platforms:** Use `OLLAMA_ORIGINS="*"` for testing, then restrict to specific domains
> - **Always verify** your setup with: `curl http://localhost:11434/api/tags`

---

## ğŸ  **LM Studio Integration**

<div align="center">

### ğŸ¯ **Local AI Power + No API Costs**

</div>

<table>
<tr>
<td width="50%">

### ğŸš€ **Why LM Studio?**
- **ğŸ”’ Fully Private** - All processing happens locally
- **ğŸ’° Zero API Costs** - No usage fees or limits
- **âš¡ Fast Response** - Direct local connection
- **ğŸ›ï¸ Model Control** - Use any compatible model
- **ğŸŒ OpenAI Compatible** - Standard API format

</td>
<td width="50%">

### ğŸ› ï¸ **Setup Requirements**
- **LM Studio App** - Download from [lmstudio.ai](https://lmstudio.ai)
- **Compatible Model** - Any chat-capable model
- **Local Server** - LM Studio server on port 1234
- **CORS Enabled** - Allow cross-origin requests

</td>
</tr>
</table>

### ğŸ“‹ **Quick Setup Guide**

> **ğŸš¨ KEY REQUIREMENT**: You **MUST** enable CORS in LM Studio for TimeCapsule-SLM to connect.

**Step 1:** ğŸ“¥ **Download LM Studio** from [lmstudio.ai](https://lmstudio.ai) and install it  
**Step 2:** ğŸ¤– **Download a Model** - Search for models like `Qwen3 0.6B`
**Step 3:** ğŸš€ **Start Local Server** - Click "Start Server" in LM Studio (port 1234)  
**Step 4:** âš™ï¸ **Enable CORS** - **IMPORTANT**: In LM Studio â†’ Settings â†’ Server â†’ Enable "CORS"  
**Step 5:** ğŸ”„ **Restart Server** - Stop and restart the LM Studio server  
**Step 6:** ğŸ’Š **Connect in TimeCapsule** - Select "ğŸ  LM Studio" from AI provider dropdown  
**Step 7:** ğŸ”Œ **Click Connect** - TimeCapsule will auto-detect your model  

### ğŸ¯ **Recommended Models**

| Model | Size | Best For | Performance |
|-------|------|----------|-------------|
| **Qwen2.5-7B-Instruct** | ~4GB | Research analysis, detailed coding responses | ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ |

---

## ğŸ”¬ **DeepResearch TimeCapsule Usage**

<div align="center">

### ğŸ§  **AI-Powered Research Generation**

</div>

### ğŸ“Š **Research Workflow**
1. **ğŸ“ Add Topics** - Define research areas with descriptions
2. **ğŸ¯ Select Type** - Choose from Academic, Market, Technology, Competitive, Trend, Literature
3. **ğŸ“ Set Depth** - Pick Overview, Detailed, or Comprehensive analysis  
4. **ğŸ¤– Generate Research** - AI creates structured, professional reports and TimeCapsules
5. **ğŸ“¤ Export Results** - Download as `.timecapsule.json` files for sharing

### ğŸ¯ **Research Types Explained**
- **ğŸ“š Academic** - Scholarly analysis with citations and methodology
- **ğŸ“ˆ Market** - Industry trends, competition, and market analysis
- **ğŸ”§ Technology** - Technical deep-dives and implementation insights  
- **ğŸ¢ Competitive** - Competitor analysis and market positioning
- **ğŸ“Š Trend** - Emerging patterns and future predictions
- **ğŸ“– Literature** - Comprehensive literature reviews and surveys

---

## ğŸ® **Playground Usage**

<div align="center">

### ğŸ¨ **Creative Coding with AI Assistance**

</div>



---

## ğŸ“„ **Project Structure**

<div align="center">

### ğŸ“ **Clean, Organized Architecture**

</div>

<table>
<tr>
<td width="50%">

### ğŸ’Š **Core Application Files**
| File | Description |
|------|-------------|
| `DeepResearch.html` | DeepResearch TimeCapsule studio |
| `Playground.html` | Playground creative AI environment |
| `Canvas.html` | Creative CodingEnvironment |
| `index.html` | Main platform homepage |
| `Script01.js` | Utility functions and helpers |

</td>
<td width="50%">

### ğŸ“‹ **Documentation & Assets**
| File | Description |
|------|-------------|
| `README.md` | This comprehensive guide |
| `CREDITS` | Algorithm attributions |
| `LICENSE` | Apache 2.0 License |
| `lib/` | Assets and design templates |

</td>
</tr>
</table>

### ğŸ“‚ **Library Structure**
- **`lib/agent/`** - Canvas AI agents
- **`lib/AIAssistant/`** - AI backend integration
- **`lib/Designs/`** - Creative coding templates
- **`lib/Pages/`** - Component libraries
- **`lib/Media/`** - Images and assets

---

## ğŸš€ **Powered By**

<div align="center">
  
  <a href="https://bubblspace.com" target="_blank">
    <img src="lib/Media/BubblLogZoomed.png" alt="BubblSpace" width="120" style="margin: 20px;">
  </a>
  
  ### **Proudly Supported by [BubblSpace](https://bubblspace.com)**
  
  *Building the future of AI-powered creativity and collaboration*
  
  [![Visit BubblSpace](https://img.shields.io/badge/ğŸŒ_Visit-BubblSpace.com-4285f4?style=for-the-badge)](https://bubblspace.com)
  
</div>

---

<div align="center">

### ğŸ§™â€â™‚ï¸ **Created with â¤ï¸ by [FireHacker](https://x.com/thefirehacker)**

**Made for researchers, creators, developers, and digital artists worldwide**

[![Twitter Follow](https://img.shields.io/twitter/follow/thefirehacker?style=social)](https://x.com/thefirehacker)
[![GitHub stars](https://img.shields.io/github/stars/thefirehacker/TimeCapsule-SLM?style=social)](https://github.com/thefirehacker/TimeCapsule-SLM)
[![Discord](https://img.shields.io/badge/ğŸ’¬_Join-Discord_Community-7289da?style=for-the-badge)](https://discord.gg/ExQ8fCv9)

---

## ğŸ’¬ **Support & Community**

<div align="center">

**Need Help? We're Here for You!**

ğŸ§ **Join our Discord Community**: [discord.gg/ExQ8fCv9](https://discord.gg/ExQ8fCv9)  
ğŸ“§ **Email Support**: [support@bubblspace.com](mailto:support@bubblspace.com)  
ğŸ› **Report Issues**: [GitHub Issues](https://github.com/thefirehacker/TimeCapsule-SLM/issues)  
ğŸ“š **Documentation**: [Main README](README.md) â€¢ [Docker Guide](DOCKER.md)  

*Get help with setup, AI integration, research workflows, and more!*

</div>

---

*â­ If you found this project helpful, please give it a star! â­*

</div>

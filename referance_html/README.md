<div align="center">
  
  <img src="https://mybubblpublic.s3.ap-south-1.amazonaws.com/TimeCapsule_03.png" alt="TimeCapsule-SLM Logo" width="180" style="border-radius: 20px; box-shadow: 0 10px 30px rgba(0,0,0,0.3);">
  
  # ğŸ’Š TimeCapsule-SLM
  
  ### *Complete AI-powered research & creative platform with DeepResearch*
  ### **Generate Novel Ideas â€¢ Build AI Content â€¢ Enable Collaborative Knowledge Discovery**
  
  [![Live Demo](https://img.shields.io/badge/ğŸš€_Live_Demo-Available-00C851?style=for-the-badge&logo=rocket)](https://timecapsule.bubblspace.com/)
  [![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg?style=for-the-badge)](https://opensource.org/licenses/Apache-2.0)
  [![AI Powered](https://img.shields.io/badge/ğŸ¤–_AI-Powered-ff6b35?style=for-the-badge)](https://timecapsule.bubblspace.com/)
  [![Made with Love](https://img.shields.io/badge/Made_with-â¤-red?style=for-the-badge)](https://x.com/thefirehacker)
  
  ### ğŸ”® **[ğŸŒŸ View Our Complete Vision & Roadmap â†’](vision.md)**
  *Discover the future of AI-powered research, our current priorities, and upcoming showcase projects*
  
</div>

---

## ğŸ”— **Quick Links**
- [ğŸ”® **Project Vision & Roadmap**](vision.md) - *Future of AI Research*
- [ğŸš€ Key Features](#-key-features-2024)
- [ğŸŒ Experience Live Now](#-experience-live-now)
- [ğŸš¦ How to Start](#-how-to-start)
- [ğŸ³ Docker Support- In Progress](DOCKER.md)
- [ğŸ”— How to Share](#-how-to-share)
- [ğŸ’¬ Join Discord Community](https://discord.gg/ExQ8fCv9)
- [ğŸ¦™ Ollama Integration](#-ollama-integration)
- [ğŸ  LM Studio Integration](#-lm-studio-integration)
- [ğŸ”¬ DeepResearch Usage](#-deepresearch-timecapsule-usage)
- [ğŸ® Playground Usage](#-playground-usage)
- [ğŸ“„ Project Structure](#-project-structure)
- [ğŸš€ Powered By](#-powered-by)

---

## ğŸš€ **Key Features **

<div align="center">

### ğŸ§  **In-Browser RAG** | ğŸ”— **TimeCapsule Sharing** | ğŸ“š **Knowledge Base** | ğŸ¤– **Local LLM Support**

</div>

<table>
<tr>
<td width="50%">

### ğŸ§  **In-Browser RAG**
Semantic search and Retrieval-Augmented Generation with your own documentsâ€”directly in your browser. **No server, no data leaves your device.**

### ğŸ“š **Knowledge Base Integration**
Upload PDFs, text, and images to build a private, searchable research knowledge base with intelligent document analysis.

</td>
<td width="50%">

### ğŸ”— **TimeCapsule Sharing**
Export and load full research sessions as `.timecapsule.json` files. **Instantly share, restore, or collaborate** on research with anyone.

### ğŸ¤– **Local LLM Support**
Use **Ollama**, **LM Studio**, **OpenAI**, and **Anthropic**â€”privacy-first, cost-effective, and lightning fast.

</td>
</tr>
</table>

---

## ğŸŒ **âœ¨ Experience Live Now! âœ¨**

<div align="center">

### **ğŸ¯ [Launch TimeCapsule-SLM â†’](https://timecapsule.bubblspace.com/)**

<table>
<tr>
<td width="50%" align="center">
  
**ğŸš€ Instant Access**

No downloads, no setup - just click and create!
Professional-grade research and creative coding in your browser.
  
</td>
<td width="50%" align="center">
  
**ğŸ¤– AI-Powered**

**Ollama**, **LM Studio** & **API** (**OpenAI**, **Anthropic**) integration for intelligent research analysis and creative code generation.
  
</td>
</tr>
</table>

### ğŸ’Š **What You Get:**

ğŸ”¬ **DeepResearch TimeCapsule** - Comprehensive AI-powered research platform  
ğŸ® **Playground** - Execute TimeCapsules with creative coding  
ğŸ§  **Triple AI Mode**: Ollama, LM Studio and APIs (OpenAI, Anthropic)  
âš™ï¸ **Custom Context Templates** for personalized AI behavior  
ğŸ“± **Responsive Design** that works on all devices  
ğŸ”„ **Seamless Navigation** between research and creative modes  
ğŸ”’ **Privacy First** with multiple local AI options  

</div>

---

## ğŸš¦ **How to Start**

<div align="center">

### ğŸ¯ **Get Research-Ready in 5 Minutes**

</div>

<table>
<tr>
<td width="33%">

### ğŸŒ **Option 1: Instant Online** *(Recommended)*
1. ğŸ¯ **Go to** [timecapsule.bubblspace.com](https://timecapsule.bubblspace.com/)
2. ğŸ”¬ **Click "DeepResearch"**
3. ğŸ¦™ **Start Ollama** (see setup below)
4. ğŸ¤– **Pull a model:** `ollama pull qwen3:0.6b`
5. ğŸ“š **Add documents** in Knowledge Manager
6. ğŸ“ **Add research topics** and click **Generate**

</td>
<td width="33%">

### ğŸ³ **Option 2: Docker** *(Easy Deploy)*
1. ğŸ“ **Clone:** `git clone https://github.com/thefirehacker/TimeCapsule-SLM`
2. ğŸ“‚ **Navigate:** `cd TimeCapsule-SLM`
3. ğŸ³ **Start:** `docker-compose --profile ai-enabled up -d`
4. ğŸŒ **Access:** `http://localhost:3000`
5. ğŸ¦™ **Pull model:** `docker exec timecapsule-ollama ollama pull qwen3:0.6b`
6. ğŸš€ **Start researching!**

</td>
<td width="33%">

### ğŸ’» **Option 3: Local Development**
1. ğŸ“ **Clone:** `git clone https://github.com/thefirehacker/TimeCapsule-SLM`
2. ğŸ“‚ **Navigate:** `cd TimeCapsule-SLM`
3. ğŸŒ **Open:** `DeepResearch.html` in browser
4. ğŸ¦™ **Setup Ollama** (see integration guide)
5. ğŸš€ **Start researching!**

</td>
</tr>
</table>

### ğŸ¦™ **Quick Ollama Setup** *(Essential for local AI)*

```bash
# 1. Install Ollama from https://ollama.ai

# 2. Pull recommended model
ollama pull qwen3:0.6b

# 3. Start with CORS enabled (CRITICAL)
OLLAMA_ORIGINS="https://timecapsule.bubblspace.com/,http://localhost:3000" ollama serve

# 4. Connect in TimeCapsule-SLM
```

> **ğŸ’¡ Pro Tip:** For best results, use **Ollama** with the `qwen3:0.6b` model. **LM Studio** and **APIs** (OpenAI, Anthropic) are also fully supported.

---

## ğŸ”— **How to Share**

<div align="center">

### ğŸ¤ **Collaborate & Share Research Instantly**

</div>

<table>
<tr>
<td width="50%" align="center">

### ğŸ“¤ **Export TimeCapsule**
1. ğŸ”¬ **Complete your research** in DeepResearch
2. ğŸ’¾ **Click "Export TimeCapsule"** button
3. ğŸ“ **Save** `.timecapsule.json` file
4. ğŸ¤ **Share** with colleagues or save for later

**Perfect for:** Research collaboration, session backup, knowledge sharing

</td>
<td width="50%" align="center">

### ğŸ“¥ **Load TimeCapsule** 
1. ğŸ“‚ **Click "Load TimeCapsule"** button
2. ğŸ—‚ï¸ **Select** `.timecapsule.json` file
3. âš¡ **Instantly restore** topics and research output
4. ğŸ”„ **Continue** where you left off

**Perfect for:** Resuming sessions, importing shared research, team collaboration

</td>
</tr>
</table>

### ğŸ¯ **TimeCapsule Features**
- **ğŸ”„ Complete Session Restore** - All topics, research results, and notes
- **ğŸ“Š Multi-Tab Support** - Research, Sources, and Notes tabs preserved
- **ğŸ¤ Team Collaboration** - Share research across teams instantly
- **ğŸ’¾ Session Backup** - Never lose your research progress
- **ğŸŒ Cross-Platform** - Works on any device with TimeCapsule-SLM

---

## ğŸ¦™ **Ollama Integration**

<div align="center">

### ğŸ¯ **Local AI Power + Privacy First**
**Complete platform-specific setup guides for macOS, Linux & Windows**

</div>

<table>
<tr>
<td width="50%">

### ğŸš€ **Why Ollama?**
- **ğŸ”’ Fully Private** - All processing happens locally
- **ğŸ’° Zero API Costs** - No usage fees or limits
- **âš¡ Lightning Fast** - Optimized GGUF models
- **ğŸ›ï¸ Model Library** - Easy model management
- **ğŸŒ REST API** - Simple integration

</td>
<td width="50%">

### ğŸ› ï¸ **Setup Requirements**
- **Ollama App** - Download from [ollama.ai](https://ollama.ai)
- **AI Model** - Any compatible GGUF model
- **CORS Enabled** - **CRITICAL** for web access
- **Port 11434** - Default Ollama server port

</td>
</tr>
</table>

---

## ğŸ **macOS Setup Guide**

### ğŸ“¥ **Step 1: Install Ollama**
```bash
# Method 1: Direct download (recommended)
# Download from https://ollama.ai and install .app

# Method 2: Homebrew
brew install ollama
```

### ğŸ¤– **Step 2: Pull a Model**
```bash
# Recommended: Fast and efficient
ollama pull qwen3:0.6b
```

### ğŸ”§ **Step 3: Start with CORS** (**CRITICAL**)
```bash
# Kill any existing processes first
pkill -f ollama

# Start with CORS enabled (for testing)
OLLAMA_ORIGINS="*" ollama serve

# For production (recommended)
OLLAMA_ORIGINS="https://timecapsule.bubblspace.com/,http://localhost:3000" ollama serve
```

### ğŸ”§ **macOS Troubleshooting**

**âŒ "Operation not permitted" Error:**
```bash
# Method 1: Use sudo
sudo pkill -f ollama

# Method 2: Activity Monitor (GUI)
# 1. Open Activity Monitor (Applications â†’ Utilities)
# 2. Search for "ollama"
# 3. Select process and click "Force Quit"

# Method 3: Homebrew service (if installed via brew)
brew services stop ollama
brew services start ollama
```

**âŒ CORS Issues:**
```bash
# 1. Stop Ollama completely
sudo pkill -f ollama

# 2. Wait 3 seconds
sleep 3

# 3. Start with CORS
OLLAMA_ORIGINS="*" ollama serve

# 4. Test connection
curl http://localhost:11434/api/tags
```

---

## ğŸ§ **Linux Setup Guide**

### ğŸ“¥ **Step 1: Install Ollama**
```bash
# Official installer (recommended)
curl -fsSL https://ollama.ai/install.sh | sh

# Or download directly from https://ollama.ai
```

### ğŸ¤– **Step 2: Pull a Model**
```bash
# Recommended model
ollama pull qwen3:0.6b
```

### ğŸ”§ **Step 3: Configure CORS with systemctl** (**CRITICAL**)

**For systemd-based Linux distributions (Ubuntu, Debian, CentOS, etc.):**

```bash
# 1. Stop any running Ollama instances
ps aux | grep ollama
sudo pkill -f ollama

# 2. Edit the ollama service configuration
sudo systemctl edit ollama.service

# 3. Add the following environment variables:
[Service]
Environment="OLLAMA_HOST=0.0.0.0"
Environment="OLLAMA_ORIGINS=*"

# For production, use specific origins:
# Environment="OLLAMA_ORIGINS=https://timecapsule.bubblspace.com/,http://localhost:3000"

# 4. Save and exit the editor (Ctrl+X, then Y, then Enter)

# 5. Reload systemd and restart ollama service
sudo systemctl daemon-reload
sudo systemctl restart ollama.service

# 6. Enable auto-start on boot (optional)
sudo systemctl enable ollama.service

# 7. Verify the service is running
sudo systemctl status ollama.service

# 8. Test the connection
curl http://localhost:11434/api/tags
```

**Alternative: Manual start (if not using systemd):**
```bash
# Stop any existing processes
sudo pkill -f ollama

# Start manually with CORS
OLLAMA_ORIGINS="*" ollama serve

# Or for production:
# OLLAMA_ORIGINS="https://timecapsule.bubblspace.com/,http://localhost:3000" ollama serve
```

### ğŸ”§ **Linux Troubleshooting**

**âŒ Service Issues:**
```bash
# Check service logs
sudo journalctl -u ollama.service -f

# Restart service
sudo systemctl restart ollama.service

# Check service status
sudo systemctl status ollama.service
```

**âŒ Permission Issues:**
```bash
# Stop with elevated permissions
sudo pkill -f ollama

# Check for lingering processes
ps aux | grep ollama

# Force kill if needed
sudo kill -9 $(pgrep ollama)
```

**âŒ CORS Configuration:**
```bash
# Verify environment variables are set
sudo systemctl show ollama.service | grep Environment

# If not set, re-edit the service:
sudo systemctl edit ollama.service
# Add Environment variables as shown above
sudo systemctl daemon-reload
sudo systemctl restart ollama.service
```

**ğŸ“š Reference:** [Ollama CORS Configuration Guide](https://objectgraph.com/blog/ollama-cors/)

---

## ğŸªŸ **Windows Setup Guide**

### ğŸ“¥ **Step 1: Install Ollama**
```powershell
# Download from https://ollama.ai and install the .exe
# Or use package manager (if available)
```

### ğŸ¤– **Step 2: Pull a Model**
```powershell
# Open Command Prompt or PowerShell
ollama pull qwen3:0.6b
```

### ğŸ”§ **Step 3: Start with CORS** (**CRITICAL**)
```powershell
# Method 1: Stop existing processes
taskkill /f /im ollama.exe

# Method 2: Start with CORS (Command Prompt)
set OLLAMA_ORIGINS=* && ollama serve

# Method 3: Start with CORS (PowerShell)
$env:OLLAMA_ORIGINS="*"; ollama serve

# For production (specific origins):
# $env:OLLAMA_ORIGINS="https://timecapsule.bubblspace.com/,http://localhost:3000"; ollama serve
```

### ğŸ”§ **Windows Troubleshooting**

**âŒ Process Issues:**
```powershell
# Method 1: Task Manager (GUI)
# 1. Open Task Manager (Ctrl+Shift+Esc)
# 2. Look for "ollama.exe" in Processes tab
# 3. Right-click and select "End task"

# Method 2: Command line
taskkill /f /im ollama.exe

# Method 3: Find by port
netstat -ano | findstr :11434
# Note the PID and kill it:
taskkill /f /pid <PID>
```

**âŒ CORS Issues:**
```powershell
# 1. Stop all ollama processes
taskkill /f /im ollama.exe

# 2. Wait 3 seconds
timeout /t 3

# 3. Start with CORS
$env:OLLAMA_ORIGINS="*"; ollama serve

# 4. Test connection (if curl is available)
curl http://localhost:11434/api/tags
```

**âŒ Environment Variables:**
```powershell
# Set permanently (requires restart)
setx OLLAMA_ORIGINS "*"

# Set for current session only
$env:OLLAMA_ORIGINS="*"
```

---

## ğŸ¯ **Universal Commands & Verification**

### ğŸ§ª **Test Your Setup**
```bash
# 1. Check if Ollama is running
curl http://localhost:11434/api/tags

# 2. List installed models
ollama list

# 3. Test model response
curl http://localhost:11434/api/generate -d '{
  "model": "qwen3:0.6b",
  "prompt": "Hello",
  "stream": false
}'
```

### ğŸ“¦ **Recommended Models**

| Model | Size | Best For | Performance |
|-------|------|----------|-------------|
| **qwen3:0.6b** | ~400MB | Fast responses, testing | ğŸŒŸğŸŒŸğŸŒŸğŸŒŸâ­ |
| **qwen2.5:3b** | ~2GB | Balanced quality/speed | ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ |
| **llama3.2:3b** | ~2GB | General purpose | ğŸŒŸğŸŒŸğŸŒŸâ­â­ |

```bash
# Pull additional models:
ollama pull qwen2.5:3b
ollama pull llama3.2:3b
```

### ğŸ†˜ **Universal Reset (All Platforms)**

**If everything fails, complete reset:**
```bash
# 1. Stop all Ollama processes
# macOS/Linux: sudo pkill -f ollama
# Windows: taskkill /f /im ollama.exe

# 2. Wait 5 seconds
sleep 5  # macOS/Linux
# timeout /t 5  # Windows

# 3. Start fresh with CORS
OLLAMA_ORIGINS="*" ollama serve
# Windows PowerShell: $env:OLLAMA_ORIGINS="*"; ollama serve

# 4. Pull a model (in new terminal)
ollama pull qwen3:0.6b

# 5. Test setup
curl http://localhost:11434/api/tags
```

> **ğŸ’¡ Pro Tips**: 
> - **Linux Users:** Use systemctl for persistent CORS configuration
> - **macOS Users:** Use Activity Monitor for stubborn processes
> - **Windows Users:** Use Task Manager or PowerShell for process management
> - **All Platforms:** Use `OLLAMA_ORIGINS="*"` for testing, then restrict to specific domains
> - **Always verify** your setup with: `curl http://localhost:11434/api/tags`

### ğŸŒ **Custom Ollama URL (Local Builds Only)**

> **âš ï¸ Note**: Custom URLs only work in local builds, not hosted version.

**Easy Setup with ollama-custom.js:**
1. **Edit Configuration File**: Open `ollama-custom.js` in the root directory
2. **Add Your IPs**: Replace the example IPs with your actual Ollama servers
   ```javascript
   customIPs: [
     "http://10.0.1.69:11434",      // Your first Ollama server
     "http://192.168.1.200:11434",  // Your second Ollama server  
     "http://172.16.0.50:9434"      // Your third Ollama server
   ]
   ```
3. **Save and Refresh**: Save the file and hard refresh your browser (Ctrl+Shift+R)

**Use in App**: Click "Connect Ollama" â†’ accept agreement â†’ Enter custom URL in popup
   - **DeepResearch**: Click "ğŸ¦™ Connect Ollama" 
   - **Playground**: Click "Connect AI" â†’ Select Ollama

**Examples**: `http://192.168.1.100:11434`, `http://localhost:9434`, `https://ollama.mydomain.com`

---

## ğŸ  **LM Studio Integration**

<div align="center">

### ğŸ¯ **Local AI Power + No API Costs**

</div>

<table>
<tr>
<td width="50%">

### ğŸš€ **Why LM Studio?**
- **ğŸ”’ Fully Private** - All processing happens locally
- **ğŸ’° Zero API Costs** - No usage fees or limits
- **âš¡ Fast Response** - Direct local connection
- **ğŸ›ï¸ Model Control** - Use any compatible model
- **ğŸŒ OpenAI Compatible** - Standard API format

</td>
<td width="50%">

### ğŸ› ï¸ **Setup Requirements**
- **LM Studio App** - Download from [lmstudio.ai](https://lmstudio.ai)
- **Compatible Model** - Any chat-capable model
- **Local Server** - LM Studio server on port 1234
- **CORS Enabled** - Allow cross-origin requests

</td>
</tr>
</table>

### ğŸ“‹ **Quick Setup Guide**

> **ğŸš¨ KEY REQUIREMENT**: You **MUST** enable CORS in LM Studio for TimeCapsule-SLM to connect.

**Step 1:** ğŸ“¥ **Download LM Studio** from [lmstudio.ai](https://lmstudio.ai) and install it  
**Step 2:** ğŸ¤– **Download a Model** - Search for models like `Qwen3 0.6B`
**Step 3:** ğŸš€ **Start Local Server** - Click "Start Server" in LM Studio (port 1234)  
**Step 4:** âš™ï¸ **Enable CORS** - **IMPORTANT**: In LM Studio â†’ Settings â†’ Server â†’ Enable "CORS"  
**Step 5:** ğŸ”„ **Restart Server** - Stop and restart the LM Studio server  
**Step 6:** ğŸ’Š **Connect in TimeCapsule** - Select "ğŸ  LM Studio" from AI provider dropdown  
**Step 7:** ğŸ”Œ **Click Connect** - TimeCapsule will auto-detect your model  

### ğŸ¯ **Recommended Models**

| Model | Size | Best For | Performance |
|-------|------|----------|-------------|
| **Qwen3 0.6B** | ~500MB | Research analysis, detailed coding responses | ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ |

---

## ğŸ”¬ **DeepResearch TimeCapsule Usage**

<div align="center">

### ğŸ§  **AI-Powered Research Generation**

</div>

### ğŸ“Š **Research Workflow**
1. **ğŸ“ Add Topics** - Define research areas with descriptions
2. **ğŸ¯ Select Type** - Choose from Academic, Market, Technology, Competitive, Trend, Literature
3. **ğŸ“ Set Depth** - Pick Overview, Detailed, or Comprehensive analysis  
4. **ğŸ¤– Generate Research** - AI creates structured, professional reports and TimeCapsules
5. **ğŸ“¤ Export Results** - Download as `.timecapsule.json` files for sharing

### ğŸ¯ **Research Types Explained**
- **ğŸ“š Academic** - Scholarly analysis with citations and methodology
- **ğŸ“ˆ Market** - Industry trends, competition, and market analysis
- **ğŸ”§ Technology** - Technical deep-dives and implementation insights  
- **ğŸ¢ Competitive** - Competitor analysis and market positioning
- **ğŸ“Š Trend** - Emerging patterns and future predictions
- **ğŸ“– Literature** - Comprehensive literature reviews and surveys

---

## ğŸ® **Playground Usage**

<div align="center">

### ğŸ¨ **Creative Coding with AI Assistance**

</div>



---

## ğŸ“„ **Project Structure**

<div align="center">

### ğŸ“ **Clean, Organized Architecture**

</div>

<table>
<tr>
<td width="50%">

### ğŸ’Š **Core Application Files**
| File | Description |
|------|-------------|
| `DeepResearch.html` | DeepResearch TimeCapsule studio |
| `Playground.html` | Playground creative AI environment |
| `Canvas.html` | Creative CodingEnvironment |
| `index.html` | Main platform homepage |
| `Script01.js` | Utility functions and helpers |

</td>
<td width="50%">

### ğŸ“‹ **Documentation & Assets**
| File | Description |
|------|-------------|
| `README.md` | This comprehensive guide |
| `CREDITS` | Algorithm attributions |
| `LICENSE` | Apache 2.0 License |
| `lib/` | Assets and design templates |

</td>
</tr>
</table>

### ğŸ“‚ **Library Structure**
- **`lib/agent/`** - Canvas AI agents
- **`lib/AIAssistant/`** - AI backend integration
- **`lib/Designs/`** - Creative coding templates
- **`lib/Pages/`** - Component libraries
- **`lib/Media/`** - Images and assets

---

## ğŸš€ **Powered By**

<div align="center">
  
  <a href="https://bubblspace.com" target="_blank">
    <img src="lib/Media/BubblLogZoomed.png" alt="BubblSpace" width="120" style="margin: 20px;">
  </a>
  
  ### **Proudly Supported by [BubblSpace](https://bubblspace.com)**
  
  *Building the future of AI-powered creativity and collaboration*
  
  [![Visit BubblSpace](https://img.shields.io/badge/ğŸŒ_Visit-BubblSpace.com-4285f4?style=for-the-badge)](https://bubblspace.com)
  
</div>

---

<div align="center">

<img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&weight=600&size=28&pause=1000&color=667EEA&background=FFFFFF00&center=true&vCenter=true&width=600&lines=Created+with+%E2%9D%A4%EF%B8%8F+by+FireHacker;AI-Powered+Research+%26+Creativity;Join+Our+Growing+Community!" alt="Typing SVG" />

### ğŸ§™â€â™‚ï¸ **Created with â¤ï¸ by [FireHacker](https://x.com/thefirehacker)**

**ğŸŒ Made for researchers, creators, developers, and digital artists worldwide**

<table>
<tr>
<td align="center" width="33%">

[![Twitter Follow](https://img.shields.io/twitter/follow/thefirehacker?style=for-the-badge&logo=twitter&logoColor=white&color=1DA1F2)](https://x.com/thefirehacker)

**ğŸ¦ Follow @thefirehacker**

</td>
<td align="center" width="33%">

<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 15px; border-radius: 12px; border: 2px solid #FFD700;">

[![GitHub stars](https://img.shields.io/github/stars/thefirehacker/TimeCapsule-SLM?style=for-the-badge&logo=github&logoColor=white&color=gold&labelColor=333)](https://github.com/thefirehacker/TimeCapsule-SLM)

**â­ STAR THIS PROJECT â­**

*Help us reach 100 stars!*

</div>

</td>
<td align="center" width="33%">

[![Discord](https://img.shields.io/badge/ğŸ’¬_Join-Discord_Community-7289da?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/ExQ8fCv9)

**ğŸ® Discord Community**

</td>
</tr>
</table>

</div>

---

<div align="center">

## ğŸ’¬ **Support & Community**

<img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&weight=500&size=22&pause=1000&color=4FACFE&background=FFFFFF00&center=true&vCenter=true&width=500&lines=Need+Help%3F+We're+Here+for+You!;Join+Our+Amazing+Community!;Get+Expert+AI+Support!" alt="Support Typing SVG" />

<table>
<tr>
<td width="25%" align="center">

### ğŸ§ **Discord Community**

[![Discord](https://img.shields.io/discord/1234567890?style=for-the-badge&logo=discord&logoColor=white&color=7289da&label=JOIN%20DISCORD)](https://discord.gg/ExQ8fCv9)

**Real-time help & discussions**  
Connect with fellow researchers!

[ğŸ’¬ **discord.gg/ExQ8fCv9**](https://discord.gg/ExQ8fCv9)

</td>
<td width="25%" align="center">

### ğŸ“§ **Email Support**

[![Email](https://img.shields.io/badge/ğŸ“§_Email-Support-EA4335?style=for-the-badge&logo=gmail&logoColor=white)](mailto:support@bubblspace.com)

**Direct technical assistance**  
Professional support team

[ğŸ“§ **support@bubblspace.com**](mailto:support@bubblspace.com)

</td>
<td width="25%" align="center">

### ğŸ› **Report Issues**

[![GitHub Issues](https://img.shields.io/github/issues/thefirehacker/TimeCapsule-SLM?style=for-the-badge&logo=github&logoColor=white&color=333)](https://github.com/thefirehacker/TimeCapsule-SLM/issues)

**Bug reports & feature requests**  
Help improve the platform

[ğŸ”§ **GitHub Issues**](https://github.com/thefirehacker/TimeCapsule-SLM/issues)

</td>
<td width="25%" align="center">

### ğŸ“š **Documentation**

[![Docs](https://img.shields.io/badge/ğŸ“š_Full-Documentation-4285f4?style=for-the-badge&logo=readme&logoColor=white)](README.md)

**Complete guides & tutorials**  
Everything you need to know

[ğŸ“– **View Docs**](README.md) â€¢ [ğŸ³ **Docker**](DOCKER.md)

</td>
</tr>
</table>

### ğŸ†˜ **Get Help With:**
ğŸ”§ **Setup & Installation** â€¢ ğŸ¤– **AI Integration** â€¢ ğŸ”¬ **Research Workflows** â€¢ ğŸ“š **Document Management** â€¢ ğŸ® **Creative Coding** â€¢ ğŸ”„ **TimeCapsule Sharing** â€¢ ğŸ› **Troubleshooting**

---

<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 15px; border: 3px solid #FFD700; margin: 20px 0;">

### â­ **LOVE THIS PROJECT? GIVE IT A STAR!** â­

<table>
<tr>
<td width="60%" align="center">

[![GitHub stars](https://img.shields.io/github/stars/thefirehacker/TimeCapsule-SLM?style=for-the-badge&logo=github&logoColor=white&color=gold&labelColor=333&label=â­%20STARS)](https://github.com/thefirehacker/TimeCapsule-SLM)

**ğŸ¯ Help us reach 100 stars and unlock new features!**

</td>
<td width="40%" align="center">

[![Star on GitHub](https://img.shields.io/badge/ğŸŒŸ_Click_to-STAR_NOW-FFD700?style=for-the-badge&logo=github&logoColor=333)](https://github.com/thefirehacker/TimeCapsule-SLM)

**âš¡ Just one click makes a huge difference!**

</td>
</tr>
</table>

**ğŸ™ Your star helps more developers discover TimeCapsule-SLM and supports continued development!**

</div>

### ğŸ¤ **Join Our Growing Community**

<table>
<tr>
<td width="33%" align="center">

**ğŸŒŸ Star Gazers**  
Join our amazing community of developers

[![GitHub stars](https://img.shields.io/github/stars/thefirehacker/TimeCapsule-SLM?style=for-the-badge&logo=github&logoColor=white&color=gold&labelColor=333&label=â­%20STARGAZERS)](https://github.com/thefirehacker/TimeCapsule-SLM/stargazers)

[![View Stargazers](https://img.shields.io/badge/ğŸ‘¥_View-All_Stargazers-FFD700?style=for-the-badge&logo=github&logoColor=333)](https://github.com/thefirehacker/TimeCapsule-SLM/stargazers)

</td>
<td width="33%" align="center">

**ğŸ´ Contributors**  
Be part of the development journey

[![Contributors](https://img.shields.io/github/contributors/thefirehacker/TimeCapsule-SLM?style=for-the-badge&logo=github&logoColor=white&color=blue&labelColor=333)](https://github.com/thefirehacker/TimeCapsule-SLM/graphs/contributors)

[![Contribute](https://img.shields.io/badge/ğŸ¤_Start-Contributing-4285f4?style=for-the-badge&logo=github&logoColor=white)](https://github.com/thefirehacker/TimeCapsule-SLM/contribute)

</td>
<td width="33%" align="center">

**ğŸ“ˆ Project Stats**  
Growing stronger every day

[![GitHub Activity](https://img.shields.io/github/commit-activity/m/thefirehacker/TimeCapsule-SLM?style=for-the-badge&logo=github&color=green&logoColor=white&labelColor=333)](https://github.com/thefirehacker/TimeCapsule-SLM/graphs/commit-activity)

[![Issues & PRs](https://img.shields.io/badge/ğŸ“Š_View-Project_Stats-28a745?style=for-the-badge&logo=github&logoColor=white)](https://github.com/thefirehacker/TimeCapsule-SLM/pulse)

</td>
</tr>
</table>

</div>

---

**ğŸ’« Thank you for being part of the TimeCapsule-SLM community! Together, we're revolutionizing AI-powered research and creativity. ğŸ’«**

</div>
